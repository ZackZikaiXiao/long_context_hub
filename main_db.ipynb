{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "214269f9-2d1e-468a-9934-2ded45ace9a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 目录\n",
    "1. [安装包](#安装包)\n",
    "2. [持续预训练](#持续预训练)\n",
    "3. [全参监督训练](#全参监督训练)\n",
    "4. [LoRA监督训练](#lora监督训练)\n",
    "5. [评估](#评估)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23bd0a0a-5107-4af4-bd06-59b51d66a624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.安装包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5c0f0bc-b4a7-4dc4-970b-8d947cf9118b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install -r requirements.txt\n",
    "%pip install \"deepspeed==0.14.0\" --upgrade\n",
    "%pip install modelscope\n",
    "%pip install flash-attn==2.5.9 --no-build-isolation\n",
    "# %DS_BUILD_CPU_ADAM=1  BUILD_UTILS=1  pip install deepspeed==0.14.3 -U\n",
    "%DS_BUILD_CPU_ADAM=1  BUILD_UTILS=1  pip install deepspeed==0.14.0 -U\n",
    "# %conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "\n",
    "# %pip install jieba\n",
    "# %pip install prettytable\n",
    "# %pip install accelerate --upgrade\n",
    "# %pip uninstall -y transformers\n",
    "# %pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14341886-ecf3-460f-be96-ae57e3898d61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee3ae722-232a-4c0f-93c8-35ffe1c89180",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 2.持续预训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f0b6bff-82f7-4cf0-b446-243660541840",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import mlflow\n",
    "import torch\n",
    "\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/root/.cache/torch_extensions/py310_cu118\"\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow.utils.databricks_utils.get_databricks_host_creds().token\n",
    "username = spark.sql(\"SELECT current_user()\").first()['current_user()']\n",
    "experiment_path = f'/Users/{username}/finetune-llama3-8b'\n",
    "mlflow.set_experiment(experiment_path)\n",
    "os.environ[\"EXPERIMENT_PATH\"] = experiment_path\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 设置路径参数\n",
    "base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context\"\n",
    "data_path = base_path + \"/data\"\n",
    "model_path = base_path + \"/models\"\n",
    "\n",
    "model_name_or_path = base_path + '/models/llama3-8B-8k'\n",
    "model_max_length = 16384 * 2\n",
    "output_dir = base_path + \"/models/llama3-8B-32k-ft\"\n",
    "data_path = base_path + \"/data/redpajama\"\n",
    "use_databricks = True\n",
    "\n",
    "# 如果输出目录不存在，创建它\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "NUM_PROCESSES = torch.cuda.device_count()\n",
    "print(f\"We're using {NUM_PROCESSES} GPUs\")\n",
    "single_node_multi_gpu_ckpt_path = TorchDistributor(num_processes=NUM_PROCESSES, local_mode=True, use_gpu=True) \\\n",
    "    .run(\"./fine-tune.py\", f'--model_name_or_path={model_name_or_path}', f\"--data_path={data_path}\",\n",
    "    f'--model_max_length={str(model_max_length)}', \n",
    "    f'--output_dir={output_dir}',\n",
    "    f'--use_databricks', use_databricks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d644bd6-d2c5-4370-b3b5-ae37beb57065",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.全参监督训练\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1e7b111-6c64-47ca-9757-8c7df3be2a1d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import mlflow\n",
    "import torch\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/root/.cache/torch_extensions/py310_cu118\"\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow.utils.databricks_utils.get_databricks_host_creds().token\n",
    "username = spark.sql(\"SELECT current_user()\").first()['current_user()']\n",
    "experiment_path = f'/Users/{username}/finetune-llama3-8b'\n",
    "mlflow.set_experiment(experiment_path)\n",
    "os.environ[\"EXPERIMENT_PATH\"] = experiment_path\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 设置参数\n",
    "base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context\"\n",
    "data_path = base_path + \"/data\"\n",
    "model_path = base_path + \"/models\"\n",
    "\n",
    "model_name_or_path = base_path + \"/models/llama3-8B-32k-ft\"\n",
    "model_max_length = 16384 * 2\n",
    "output_dir = base_path + \"/models/llama3-8B-32k-ft-sft\"\n",
    "filter_mode = \"all\"\n",
    "data_path = base_path + \"/data/LongAlpaca-12k.json\"\n",
    "use_databricks = True\n",
    "\n",
    "# 如果输出目录不存在，创建它\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "NUM_PROCESSES = torch.cuda.device_count()\n",
    "print(f\"We're using {NUM_PROCESSES} GPUs\")\n",
    "single_node_multi_gpu_ckpt_path = TorchDistributor(num_processes=NUM_PROCESSES, local_mode=True, use_gpu=True) \\\n",
    "    .run(\"./supervised-fine-tune.py\", f'--model_name_or_path={model_name_or_path}', f\"--data_path={data_path}\",\n",
    "    f'--model_max_length={str(model_max_length)}', \n",
    "    f'--output_dir={output_dir}',\n",
    "    f'--use_databricks', use_databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1377c25-0a97-48cd-b8a8-51e3b8a70f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 4.LoRA监督训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7937dd45-40ab-4540-b776-39e2af190ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Train\n",
    "import os\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import mlflow\n",
    "import torch\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/root/.cache/torch_extensions/py310_cu118\"\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow.utils.databricks_utils.get_databricks_host_creds().token\n",
    "username = spark.sql(\"SELECT current_user()\").first()['current_user()']\n",
    "experiment_path = f'/Users/{username}/finetune-llama3-8b'\n",
    "mlflow.set_experiment(experiment_path)\n",
    "os.environ[\"EXPERIMENT_PATH\"] = experiment_path\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 设置参数\n",
    "base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context\"\n",
    "data_path = base_path + \"/data\"\n",
    "model_path = base_path + \"/models\"\n",
    "\n",
    "model_name_or_path = base_path + \"/models/llama3-8B-8k\"\n",
    "model_max_length = 16384 * 2\n",
    "output_dir = base_path + \"/models/llama3-8B-32k-sft-lora-adapter-resampling-full-data\"\n",
    "data_path = base_path + \"/data/LongAlpaca-12k.json\"\n",
    "use_databricks = True\n",
    "\n",
    "# 如果输出目录不存在，创建它\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "NUM_PROCESSES = torch.cuda.device_count()\n",
    "print(f\"We're using {NUM_PROCESSES} GPUs\")\n",
    "single_node_multi_gpu_ckpt_path = TorchDistributor(num_processes=NUM_PROCESSES, local_mode=True, use_gpu=True) \\\n",
    "    .run(\"./supervised-fine-tune-lora.py\", f'--model_name_or_path={model_name_or_path}', f\"--data_path={data_path}\",\n",
    "    f'--model_max_length={str(model_max_length)}', \n",
    "    f'--output_dir={output_dir}',\n",
    "    f'--use_databricks', use_databricks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9987ceca-61a1-4cdd-9b1f-684a83e391ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merge\n",
    "base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context\"\n",
    "steps_list = [\"step-490\", \"step-686\", \"step-882\"]\n",
    "context_size = 16384 * 2\n",
    "cache_dir = base_path + \"/cache_dir\"\n",
    "\n",
    "base_model = base_path + \"/models/llama3-8B-8k\"\n",
    "peft_model = base_path + \"/models/llama3-8B-32k-sft-lora-adapter-resampling\" + \"/step-98\" ######### 需要修改\n",
    "save_path = base_path + \"/models/llama3-8B-32k-sft-lora-step-98\"   ######### 需要修改\n",
    "\n",
    "command = f\"python merge_lora_weights_and_save_hf_model.py --base_model {base_model} --peft_model {peft_model} --context_size {context_size} --save_path {save_path} --cache_dir {cache_dir}\"\n",
    "print(f\"Running command: {command}\")\n",
    "!{command}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.LoRAMEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.ml.torch.distributor import TorchDistributor\n",
    "import mlflow\n",
    "import torch\n",
    "os.environ[\"NCCL_DEBUG\"] = \"INFO\"\n",
    "os.environ[\"NCCL_IGNORE_DISABLED_P2P\"] = \"1\"\n",
    "os.environ[\"NCCL_P2P_DISABLE\"] = \"1\"\n",
    "os.environ[\"TORCH_EXTENSIONS_DIR\"] = \"/root/.cache/torch_extensions/py310_cu118\"\n",
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = host\n",
    "os.environ[\"DATABRICKS_TOKEN\"] = mlflow.utils.databricks_utils.get_databricks_host_creds().token\n",
    "username = spark.sql(\"SELECT current_user()\").first()['current_user()']\n",
    "experiment_path = f'/Users/{username}/finetune-llama3-8b'\n",
    "mlflow.set_experiment(experiment_path)\n",
    "os.environ[\"EXPERIMENT_PATH\"] = experiment_path\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "\n",
    "# 设置参数\n",
    "base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context\"\n",
    "data_path = base_path + \"/data\"\n",
    "model_path = base_path + \"/models\"\n",
    "\n",
    "model_name_or_path = base_path + \"/models/llama3-8B-8k\"\n",
    "model_max_length = 16384 * 2\n",
    "output_dir = base_path + \"/models/llama3-8B-32k-sft-loramel\"\n",
    "data_path = base_path + \"/data/LongAlpaca-12k.json\"\n",
    "use_databricks = True\n",
    "\n",
    "# 如果输出目录不存在，创建它\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "NUM_PROCESSES = torch.cuda.device_count()\n",
    "print(f\"We're using {NUM_PROCESSES} GPUs\")\n",
    "single_node_multi_gpu_ckpt_path = TorchDistributor(num_processes=NUM_PROCESSES, local_mode=True, use_gpu=True) \\\n",
    "    .run(\"./supervised-fine-tune-LoRAMEL.py\", f'--model_name_or_path={model_name_or_path}', f\"--data_path={data_path}\",\n",
    "    f'--model_max_length={str(model_max_length)}', \n",
    "    f'--output_dir={output_dir}',\n",
    "    f'--use_databricks', use_databricks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9bc2ab2-b206-4744-bc53-df55519fb280",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 6.评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0328c225-176c-4e29-8c1e-bd4d533083b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "# # llama3-8B-32k-ft\n",
    "import os\n",
    "steps_list = [\"step-490\", \"step-686\", \"step-882\"]\n",
    "base_path = \"/Volumes/main/default/default_volume/erikyzzhang/long_context\"\n",
    "data_path = base_path + \"/longbench/data\"\n",
    "longbench_dir = base_path + \"/longbench\"\n",
    "port = 12356\n",
    "maxlen = 31500\n",
    "use_databricks = True\n",
    "\n",
    "model_name = \"llama3-8B-32k-sft-lora-490\"       ######### 需要修改\n",
    "model_path = base_path + \"/models/llama3-8B-32k-sft-lora-step-490\"     ######### 需要修改\n",
    "\n",
    "\n",
    "if not os.path.exists(longbench_dir):\n",
    "    os.makedirs(longbench_dir)\n",
    "\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 python LongBench/pred_db.py --model_path {model_path} --model_name {model_name} --data_path {data_path} --longbench_dir {longbench_dir} --maxlen {maxlen} --port {port}\n",
    "!python ./LongBench/eval_db.py --model_name {model_name} --longbench_dir {longbench_dir}\n",
    "!python ./LongBench/analysis_db.py --longbench_dir {longbench_dir}"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "main_db",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
